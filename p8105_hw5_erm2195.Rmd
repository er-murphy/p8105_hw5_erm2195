---
title: "P8105 Homework 5"
output: github_document
date: "2023-11-13"
---

## Library and Theme Setup
Loading the `tidyverse` and `viridis` libraries for later use.

```{r load_libraries}
library(tidyverse)
library(viridis)
```

```{r, include = FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

## Importing and Cleaning Data

```{r}
homicides = 
  read_csv("Data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(str_to_title(city), ", ", str_to_upper(state)),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"      
    )
  )

```

The `homicides` dataframe has `r nrow(homicides)` observations, with variables including victim name, race, and age; the date of the homicide report; and the city and state in which the homicide was located.

## Grouping by Location

Now grouping within cities and summarizing to produce the total number of homicides and the number that are solved per `city_state`. 

```{r}

homicides_by_city = 
  homicides |> 
  group_by(city_state) |> 
  summarize(
    num_homicides = n(),
    num_unsolved = sum(resolution == "unsolved")
  )

```

## Estimating the Proportion of Unsolved Homicides in Baltimore

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}

unsolved_baltimore = 
  prop.test(
    x = filter(homicides_by_city, city_state == "Baltimore, MD") |> pull(num_unsolved),
    n = filter(homicides_by_city, city_state == "Baltimore, MD") |> pull(num_homicides))

broom::tidy(unsolved_baltimore) |>  
  knitr::kable(digits = 3)

```

## Estimating the Proportion of Unsolved Homicides in All Cities

Can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset.

```{r}
homicide_results = 
  homicides_by_city |> 
  mutate(
    prop_tests = map2(num_unsolved, num_homicides, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |> 
  select(-prop_tests) |> 
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate))

```

## Plotting the Proportion of Unsolved Homicides Per City

Finally, a plot showing the estimate (and CI) of the proportion of unsolved homicides per city:

```{r}
homicide_results |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |>  
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# Problem 2

## List of Files and Reading CSV Function

First, I'm creating a list of all of the file names of interest called `participant_ids`. Next, I run all of the files listed in `participant_ids` through a function called `read_participant`. This function reads in the csv with the argument filename as a `tibble`, cleans up the names, makes the observations "tidy", and then spits out the data as a dataframe called `participant_data`.

```{r}
participant_ids = 
  list.files(path = "Data/Problem 2/", full.names = FALSE)

read_participant = function(name) {
  
  if (!is.character(name)) {
    stop("Argument x should be a character/string")
  }
  
  filepath = str_c("Data/Problem 2/", name)
  
  participant_data = 
    read_csv(filepath) |> 
    as_tibble() |> 
    janitor::clean_names() |> 
    pivot_longer(
      everything(),
      names_to = "week",
      names_prefix = "week_",
      values_to = "value"
    ) |> 
    mutate(
      week = as.numeric(week),
      value = as.numeric(value)
    )
  
  participant_data
}
  
```

## Combining All Participants Into a Dataframe

With my function to read in the csv files created, I can now move on to combining all participants into a single dataframe. First, I'm creating this single dataframe with the name `all_participants`. One column holds all of the participant ids pulled from the list created above, modified to remove to the file extension. After running each participants through the `read_participant` function and saving the result to a new variable called `summary`, I unnest the summary dataframe for each participant. 

```{r}
all_participants = 
  tibble(
    subject_ID = participant_ids,
    summary = map(participant_ids, read_participant)
  ) |> 
  mutate(
    arm = as.factor(if_else(str_detect(subject_ID, "con"), "Control", "Experimental")),
    subject_ID = str_replace_all(subject_ID, ".csv", "")
  ) |> 
  select(subject_ID, arm, everything()) |> 
  unnest(cols = "summary")
```

## Plotting Observations by Group

```{r}
ggplot(all_participants, aes(x = week, y = value, group = subject_ID, color = arm)) +
  geom_line(alpha = .5) +
  labs(
    title = "Participant Observation Values Over Time, by Study Arm",
    x = "Week",
    y = "Value",
    color = "Study Arm"
  )
```

From the plot, we can see that participants in the control arm have, on average, lower values than participants in the experimental arm. The difference in values between the study arms is smallest earlier in the study but expands as the weeks progress. By week 7, every single participant in the experimental arm has a higher value than those in the control arm, and this lead further grows in week 8. 

# Problem 3

## Creating `sim_power` Function

This function takes an entered sample size, mu, and sigma as arguments. After randomly generating normally distributed observations following the argument specifications, it conducts a two-sided t-test for whether the mu_hat of the sample is significantly different from the true mu (provided as an argument). The function returns a dataframe with 3 variables describing the selected sample and t-test results: `mu_hat`, `p_value`, and `reject_null`.

```{r}
sim_power = function(n, mu, sigma) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  
  sim_data = 
    tidy(t.test(sim_data, alternative = "two.sided", mu = 0, conf.level = 0.95)) |> 
    select(estimate, p.value) |> 
    janitor::clean_names() |> 
    rename(
      mu_hat = estimate
    ) |> 
    mutate(
      reject_null = as.factor(if_else(p_value < 0.05, "Yes", "No")),
      p_value = round(p_value, digits = 4)
    )
}
```

## Generating 5000 Datasets Per Mu From 0 to 6

Now, using the above function, we will generate 5,000 separate samples (datasets) for each mu ranging from 0 to 6. Each observation contains information about the true mu used for the sample, iteration number, and resulting mu_hat, p-value of the t-test, and whether the null is rejected for that sample. This combined dataset includes 35,000 observations (5,000 for each of the 7 mu values).

```{r, cache = TRUE}
sim_power_results = 
  expand_grid(
    mu = 0:6,
    iteration = 1:5000
  )  |> 
  mutate(
    t_test = map(mu, \(mu) sim_power(n = 30, sigma = 2, mu = mu))
  ) |> 
  unnest(t_test)
```

## Plotting the Proportion of Null Rejections by Mu Value

Making a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.

```{r}

sim_power_results |> 
  group_by(mu) |> 
  summarize(
    power = mean(reject_null == "Yes")
  ) |> 
  ggplot(aes(x = mu, y = power)) + 
  geom_line(alpha = .5) +
  labs(
    title = "Proportion of Samples Rejecting the Null at Different μ Values",
    x = "True μ",
    y = "Porportion of Samples Rejecting the Null (Power)"
  )

```

From the plot, we can see how the association between effect size and power is positive. For true mu values between 0 and 2, the power, or proportion of samples in which the null was rejected as a result of the two-sample t-test, increased as the true mu increased. This makes sense, because as the true mean value in the population from which the samples are being generated gets further away from 0, our ability to detect a statistically significant difference between the sample mean and 0 improves. Around a true mu value of 2, the power reaches the maximum value of 1, and it holds steady at this value as the true mu gets further and further away from 0.

## Plotting the Proportion of Null Rejections by Mu Value

Making a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis.

```{r}
sim_power_results |> 
  group_by(mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat)
  ) |> 
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_line(alpha = .5) +
  labs(
    title = "Average μ̂ By True μ",
    x = "True μ",
    y = "Average μ̂ (from 5000 samples)"
  )

```

Now, making the same plot as above, but restricted to only samples for which the null was rejected.

```{r}
sim_power_results |> 
  filter(reject_null == "Yes") |> 
  group_by(mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat)
  ) |> 
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_line(alpha = .5) +
  labs(
    title = "Average μ̂ By True μ For Samples Rejecting the Null",
    x = "True μ",
    y = "Average μ̂"
  )

```

The sample average of μ̂ across tests for which the null is rejected is approximately equal to the true value of μ. This is because tests for which the null is rejected are those with a μ̂ value that is significantly different from 0. As the true μ from which the sample is generated gets further and further away from null, the samples with a μ̂ that most closely approximates that true μ are the ones that are more likely to reject the null value. So, once the true μ value deviates from 0, our filtering essentially means that we only include samples with a μ̂ that approximates the true μ, resulting in a similarly approximate mean μ̂.


